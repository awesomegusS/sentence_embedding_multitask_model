{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "044939d2ee33471eb776370c0d867529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07b9e1db6974478a82a38436ba249858",
              "IPY_MODEL_c51ddfb73af840eb9d6af9f2dfdd8386",
              "IPY_MODEL_f21bcdea8d8b4652af6b77f6938ce4a8"
            ],
            "layout": "IPY_MODEL_b63cf1d036e445369f37b027a0a01048"
          }
        },
        "07b9e1db6974478a82a38436ba249858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18510caaad6a472c927291d1fb573891",
            "placeholder": "​",
            "style": "IPY_MODEL_42bf04d2caf244588c4ebec512ab6313",
            "value": "model.safetensors: 100%"
          }
        },
        "c51ddfb73af840eb9d6af9f2dfdd8386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16964ba827d84c32a19edcd15b6c17d8",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54f676f5708b4f618a285804a2e3f5b5",
            "value": 440449768
          }
        },
        "f21bcdea8d8b4652af6b77f6938ce4a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_802a27cbbdaf42fe8bab77014bbfa665",
            "placeholder": "​",
            "style": "IPY_MODEL_f0d29b1b861e4ac99f58c846cb7649c7",
            "value": " 440M/440M [00:05&lt;00:00, 82.0MB/s]"
          }
        },
        "b63cf1d036e445369f37b027a0a01048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18510caaad6a472c927291d1fb573891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42bf04d2caf244588c4ebec512ab6313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16964ba827d84c32a19edcd15b6c17d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f676f5708b4f618a285804a2e3f5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "802a27cbbdaf42fe8bab77014bbfa665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0d29b1b861e4ac99f58c846cb7649c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset, random_split"
      ],
      "metadata": {
        "id": "ciKeF3Vb3IUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Make fixed length embeddings for input sentences"
      ],
      "metadata": {
        "id": "c1LqSmBs1o3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Mean pooling was implmented to achieve a fixed-length embedding for any input sentence. The BERT model is an encoder that produces embedding vectors for every token in a sentence. Instead of stopping at this output where we have a vector for each token, we add a mean pooling layer at the output to take the mean of the vectors which then results in a fixed length vector.\n",
        "* The vector length is of a fixed size because the seq_length/context length/embedding vector size for BERT is fixed at 768, so when you average n 768 vectors you get 1 final fixed vector of size 768 which is our fixed-length embedding vector\n",
        "\n"
      ],
      "metadata": {
        "id": "-1FArWcHE-fw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkkyn5fovT-U"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize the tokenizer and model from Hugging Face's model hub\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Define a function to encode sentences and obtain fixed-length embeddings\n",
        "def encode_sentences(sentences):\n",
        "    # Tokenize the sentences and convert them into token IDs and attention masks\n",
        "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # The model's outputs includes hidden states of the last layer\n",
        "    last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "    # Perform mean pooling over the token embeddings to get a fixed-length sentence embedding\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() # have the att mask match the shape of the last_hidden_state\n",
        "    sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1) # multiply each token embedding by its attention mask to zero out the padding tokens\n",
        "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "    sentence_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "    return sentence_embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences to test the model\n",
        "sample_sentences = [\n",
        "    \"Transformers are powerful models for natural language processing.\",\n",
        "    \"They are widely used for various tasks, such as translation and sentiment analysis.\",\n",
        "    \"I love ML\",\n",
        "    \"Dakota Favors Rocks!\"\n",
        "]\n",
        "[print(i) for i in sample_sentences]\n",
        "\n",
        "print()\n",
        "\n",
        "# Encode the sentences and print the embeddings\n",
        "sentence_embeddings = encode_sentences(sample_sentences)\n",
        "print(f\"Sentence Embeddings:\\n{sentence_embeddings}\")\n",
        "print(sentence_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2Y7cq4E18N",
        "outputId": "b98f4386-8653-4543-e63f-375bc1c8cf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are powerful models for natural language processing.\n",
            "They are widely used for various tasks, such as translation and sentiment analysis.\n",
            "I love ML\n",
            "Dakota Favors Rocks!\n",
            "\n",
            "Sentence Embeddings:\n",
            "tensor([[ 0.1937, -0.1926,  0.1387,  ..., -0.4427, -0.5905,  0.1295],\n",
            "        [ 0.0890, -0.1854, -0.0093,  ..., -0.5218, -0.2757,  0.3439],\n",
            "        [ 0.2508,  0.3918,  0.2517,  ..., -0.2790,  0.0437, -0.0609],\n",
            "        [ 0.1860, -0.2024,  0.0250,  ...,  0.2248,  0.0859,  0.0298]])\n",
            "torch.Size([4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Expound from task1 and make a model with multi-task capabilites"
      ],
      "metadata": {
        "id": "9Ivd5ZV61-DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To support/incoporate multitask learning into our already existing model, we simply add two neural network heads whose inputs are the output of our mean pooling layer.\n",
        "We can modify these neural networks to optimize/improve performance. Adding these networks was the major change in adding multitask learning capabilities to our existing model."
      ],
      "metadata": {
        "id": "tGCpbF3U5lbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model"
      ],
      "metadata": {
        "id": "-rriZXI-d0H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", num_classes_task1=4, num_classes_task2=3):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "\n",
        "        # Shared transformer backbone\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "        # Task-specific heads\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes_task1)  # Adjust output size to the number of classes in Task 1\n",
        "        )\n",
        "\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes_task2)  # Adjust output size to the number of classes in Task 2\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Pass through shared transformer model\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Use mean pooling to get sentence embeddings\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "        sentence_embedding = sum_embeddings / sum_mask  # Mean pooling across token embedding vectors\n",
        "\n",
        "        # Pass the pooled embedding through each task-specific head\n",
        "        classification_logits = self.classification_head(sentence_embedding)\n",
        "        sentiment_logits = self.sentiment_head(sentence_embedding)\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        classification_probs = torch.softmax(classification_logits, dim=1)\n",
        "        sentiment_probs = torch.softmax(sentiment_logits, dim=1)\n",
        "\n",
        "        return classification_logits, sentiment_logits\n"
      ],
      "metadata": {
        "id": "IGQqyNbEnH1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets visualize some test sample sentences prior to training"
      ],
      "metadata": {
        "id": "rym7di7t6aTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, tokenizer, and example input\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = MultiTaskModel(model_name=model_name, num_classes_task1=4, num_classes_task2=3)\n",
        "\n",
        "# Example input sentences\n",
        "sentences = [\"The underdog team pulled off an incredible victory in the championship game!\",\n",
        "            \"The new policy has received widespread praise for addressing environmental issues.\",\n",
        "            \"The latest Marvel movie was a thrilling experience with breathtaking visuals.\",\n",
        "            \"The update introduced several bugs, making the app nearly unusable.\"]\n",
        "[print(i) for i in sentences]\n",
        "print()\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(sentences,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass\n",
        "classification_logits, sentiment_logits = model(input_ids=inputs[\"input_ids\"],\n",
        "                                                attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "# Output\n",
        "print(f\"Classification Logits:\\n{classification_logits}\")\n",
        "print()\n",
        "print(f\"Sentiment Logits:\\n{sentiment_logits}\")\n",
        "print()\n",
        "print(f\"Predictions: {torch.argmax(classification_logits, dim=1)}\")\n",
        "print(f\"Predictions: {torch.argmax(sentiment_logits, dim=1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4DiJJPab67l",
        "outputId": "7cfa561a-9fbb-4476-80a4-39637f27a143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The underdog team pulled off an incredible victory in the championship game!\n",
            "The new policy has received widespread praise for addressing environmental issues.\n",
            "The latest Marvel movie was a thrilling experience with breathtaking visuals.\n",
            "The update introduced several bugs, making the app nearly unusable.\n",
            "\n",
            "Classification Logits:\n",
            "tensor([[ 0.0114,  0.0303,  0.0204,  0.0879],\n",
            "        [ 0.0924, -0.0323,  0.0219,  0.1392],\n",
            "        [ 0.0115,  0.0479,  0.0034,  0.0806],\n",
            "        [ 0.1248, -0.1183,  0.0250,  0.1327]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Sentiment Logits:\n",
            "tensor([[0.3027, 0.1047, 0.0845],\n",
            "        [0.1519, 0.1013, 0.0644],\n",
            "        [0.2066, 0.1410, 0.1427],\n",
            "        [0.1156, 0.1983, 0.1260]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Predictions: tensor([3, 3, 3, 3])\n",
            "Predictions: tensor([0, 0, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Dataset"
      ],
      "metadata": {
        "id": "PU9kbdjMdu9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets:\n",
        "\n",
        "* create a sample dataset\n",
        "* perform five epochs of training on the data\n",
        "* obtain loss\n",
        "\n",
        "**n/b**:\n",
        "\n",
        "Task 1: Classification [sentence categories:- Entertainment(0), Sports(1), Politics(2), Technology(3)]\n",
        "\n",
        "Task 2: Sentiment [sentiment categories:- negative(0), neutral(1), postive(2)]"
      ],
      "metadata": {
        "id": "-7lhtqUSAdNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, sentences, labels_task1, labels_task2, tokenizer, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.labels_task1 = labels_task1\n",
        "        self.labels_task2 = labels_task2\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label_task1 = self.labels_task1[idx]\n",
        "        label_task2 = self.labels_task2[idx]\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label_task1\": torch.tensor(label_task1),\n",
        "            \"label_task2\": torch.tensor(label_task2),\n",
        "        }"
      ],
      "metadata": {
        "id": "IV6MgFGCBJfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize tokenizer and example dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "sentences = [\n",
        "    \"The movie was fantastic!\",\n",
        "    \"The game was boring.\",\n",
        "    \"Who is the president of the United States?\",\n",
        "    \"This movie is amazing!\",\n",
        "    \"What are the latest news in tech?\",\n",
        "    \"Who won the football game?\",\n",
        "    \"This new tv series 'the penguin' is very good, it has a high IMDB rating\"\n",
        "]\n",
        "labels_task1 = [0, 1, 2, 0, 3, 1, 0]  # Task 1: Classification (sentence categories: 0 = Entertainment, 1 = Sports, 2 = Politics, 3 = Technology)\n",
        "labels_task2 = [2, 0, 1, 2, 0, 1, 2]  # Task 2: Sentiment (sentiment categories: 0 = negative, 1 = neutral, 2 = postive)\n",
        "\n",
        "\n",
        "\n",
        "dataset = MultiTaskDataset(sentences, labels_task1, labels_task2, tokenizer)\n",
        "\n",
        "# split dataset\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.7 * dataset_size)  # 70% for training\n",
        "val_size = dataset_size - train_size  # 30% for validation\n",
        "gen = torch.Generator().manual_seed(1442) # for reproducibility\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=gen)\n",
        "\n",
        "# create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
      ],
      "metadata": {
        "id": "qLFwDjO9Mx6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "  print(batch[\"input_ids\"].shape)\n",
        "  print(batch[\"attention_mask\"].shape)\n",
        "  print(batch[\"label_task1\"].shape)\n",
        "  print(batch[\"label_task2\"].shape)\n",
        "\n",
        "print()\n",
        "\n",
        "for batch in val_dataloader:\n",
        "  print(batch[\"input_ids\"].shape)\n",
        "  print(batch[\"attention_mask\"].shape)\n",
        "  print(batch[\"label_task1\"].shape)\n",
        "  print(batch[\"label_task2\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whqAAguvQvEs",
        "outputId": "d45c221d-f4b1-4dad-cb38-53c4c1ed72ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128])\n",
            "torch.Size([2, 128])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([2, 128])\n",
            "torch.Size([2, 128])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "\n",
            "torch.Size([2, 128])\n",
            "torch.Size([2, 128])\n",
            "torch.Size([2])\n",
            "torch.Size([2])\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1, 128])\n",
            "torch.Size([1])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training & Eval"
      ],
      "metadata": {
        "id": "tzCDrjmMdqAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = MultiTaskModel(num_classes_task1=4, num_classes_task2=3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task2 = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "1-1-n6aDR1bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Eval loop\n",
        "\n",
        "num_epochs = 6\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_total_loss = 0\n",
        "    val_total_loss = 0\n",
        "    correct_task1 = 0 # for accuracy calculation\n",
        "    correct_task2 = 0 # for accuracy calculation\n",
        "    total_samples = 0 # for accuracy calculation\n",
        "\n",
        "    # train\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels_task1 = batch[\"label_task1\"].to(device)\n",
        "        labels_task2 = batch[\"label_task2\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        classification_logits, sentiment_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute losses for each task\n",
        "        loss_task1 = criterion_task1(classification_logits, labels_task1)\n",
        "        loss_task2 = criterion_task2(sentiment_logits, labels_task2)\n",
        "\n",
        "        # Combine the losses\n",
        "        combined_loss = loss_task1 + loss_task2\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_total_loss += combined_loss.item()\n",
        "\n",
        "    train_avg_loss = train_total_loss / len(train_dataloader)\n",
        "\n",
        "    # eval\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      for batch in val_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels_task1 = batch[\"label_task1\"].to(device)\n",
        "        labels_task2 = batch[\"label_task2\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        classification_logits, sentiment_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_task1 = criterion_task1(classification_logits, labels_task1)\n",
        "        loss_task2 = criterion_task2(sentiment_logits, labels_task2)\n",
        "        combined_loss = loss_task1 + loss_task2\n",
        "        val_total_loss += combined_loss.item()\n",
        "\n",
        "        # Calculate predictions\n",
        "        predictions_task1 = torch.argmax(classification_logits, dim=1)\n",
        "        predictions_task2 = torch.argmax(sentiment_logits, dim=1)\n",
        "\n",
        "        # Update correct counts for accuracy calculation\n",
        "        correct_task1 += (predictions_task1 == labels_task1).sum().item()\n",
        "        correct_task2 += (predictions_task2 == labels_task2).sum().item()\n",
        "        total_samples += labels_task1.size(0)\n",
        "\n",
        "      # Calculate average loss and accuracy\n",
        "      val_avg_loss = val_total_loss / len(val_dataloader)\n",
        "      accuracy_task1 = correct_task1 / total_samples\n",
        "      accuracy_task2 = correct_task2 / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_avg_loss:.4f}, Val Loss: {val_avg_loss:.4f}, Accuracy Task 1: {accuracy_task1:.4f}, Accuracy Task 2: {accuracy_task2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmniW6jBSoR7",
        "outputId": "8a4f9f02-d35b-43a0-8f71-4842e1cf2749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6, Train Loss: 2.5255, Val Loss: 2.4704, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.3333\n",
            "Epoch 2/6, Train Loss: 2.4225, Val Loss: 2.4467, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.3333\n",
            "Epoch 3/6, Train Loss: 2.3524, Val Loss: 2.4260, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.3333\n",
            "Epoch 4/6, Train Loss: 2.2099, Val Loss: 2.4020, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.3333\n",
            "Epoch 5/6, Train Loss: 2.1313, Val Loss: 2.3800, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.6667\n",
            "Epoch 6/6, Train Loss: 2.0498, Val Loss: 2.3561, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets see the model results on the same test sentences from last time"
      ],
      "metadata": {
        "id": "qp60cqXl8Mbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the, tokenizer, and example input\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example input sentences\n",
        "sentences = [\"The underdog team pulled off an incredible victory in the championship game!\",\n",
        "            \"The new policy has received widespread praise for addressing environmental issues.\",\n",
        "            \"The latest Marvel movie was a thrilling experience with breathtaking visuals.\",\n",
        "            \"The update introduced several bugs, making the app nearly unusable.\"]\n",
        "[print(i) for i in sentences]\n",
        "print()\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(sentences,\n",
        "            max_length=128,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass\n",
        "model.eval()\n",
        "classification_logits, sentiment_logits = model(input_ids=inputs[\"input_ids\"],\n",
        "                                                attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "# Output\n",
        "print(f\"Classification Logits:\\n{classification_logits}\")\n",
        "print()\n",
        "print(f\"Sentiment Logits:\\n{sentiment_logits}\")\n",
        "print()\n",
        "print(f\"Sentence Classification Predictions: {torch.argmax(classification_logits, dim=1)}\")\n",
        "print(f\"Sentiment Predictions: {torch.argmax(sentiment_logits, dim=1)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2-a0NxOTPRF",
        "outputId": "3874bb6a-a958-48d3-ffb8-b53840441911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The underdog team pulled off an incredible victory in the championship game!\n",
            "The new policy has received widespread praise for addressing environmental issues.\n",
            "The latest Marvel movie was a thrilling experience with breathtaking visuals.\n",
            "The update introduced several bugs, making the app nearly unusable.\n",
            "\n",
            "Classification Logits:\n",
            "tensor([[ 0.2231,  0.1663, -0.1411, -0.1210],\n",
            "        [ 0.1068,  0.0857, -0.1120, -0.0523],\n",
            "        [ 0.2597,  0.0160, -0.1406, -0.0841],\n",
            "        [ 0.1937, -0.0676, -0.1111, -0.0796]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Sentiment Logits:\n",
            "tensor([[-0.0496,  0.0575, -0.0610],\n",
            "        [ 0.0081, -0.1173, -0.0834],\n",
            "        [-0.0371, -0.0248,  0.0296],\n",
            "        [ 0.0675,  0.0008, -0.1106]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Sentence Classification Predictions: tensor([0, 0, 0, 0])\n",
            "Sentiment Predictions: tensor([1, 0, 2, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Training Considerations"
      ],
      "metadata": {
        "id": "G6cLfsUyTlSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freezing Scenarios"
      ],
      "metadata": {
        "id": "E7rR25S4YmVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Freezing the entire network parameters.\n",
        "\n",
        "**Implications**: This means the model's parameters would not be updated during training, in other words there would be training of the network and the model would basically rely on the pre-trained weights.\n",
        "\n",
        "**Advantages**: This can be advantageous when we have a very small dataset and want to avoid overfitting. Since we are not performing any training and only doing inference using the pretrained models weights, it's more computationally efficient and less costly too, since theres no gradient calculations for backpropagation\n",
        "\n",
        "**How it should be trained:** Here training doesn't happen cause all the parameters are frozen."
      ],
      "metadata": {
        "id": "01chsRYE-78q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Freezing only the transformer backbone\n",
        "\n",
        "**Implications:** If just the transformer backbone is frozen and we train just the heads, we can see the transformer backbone as an encoder or feature extractor that helps yield the fixed-length embedding vectors to be used by our several task heads.\n",
        "\n",
        "**Advantages:** Training is more efficient and less costly computationally since we are updating only task head parameters during backpropagation. It improves the learning done the task heads because we only updating those parameters. It could eliminate a vanishing gradient problem if existent. It also eliminates the risk of degrading the pre-trained model general knowledge usually caused by fine-tuning\n",
        "\n",
        "**How it should be trained:** Since we are optimizing only the task-specific heads, I'll consider the following ideas when setting the following hyperparemeters:\n",
        "* I'll use a relatively higher learning rate since only small task-specific heads are being updated e.g. 1e-3 or 1e-2\n",
        "* I'll use larger batch sizes since freezing the transformer backbone frees up some memory for us\n",
        "* I'll use Adam or AdamW as choice of optimizer since we are making small scale parameter updates\n",
        "* I'll make sure to regualarize properly since the small task-specific heads are prone to overfitting due to the backbone embeddigs.\n",
        "* Won't use that many epochs, will use 5-19 epochs because the backbone already provides robust embeddings for the sentences.\n",
        "\n",
        "In terms of hyperparameter optimization, after defining my hyperparameter search space, I'll use either random search or grid search for search and clear ML for easy hyperparameter optimzation and experimentation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kWEJ5a7d-9e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Freezing either of the task heads\n",
        "\n",
        "**Implications:** This allows one task head to leverage learned represnetations from the pre-trained model. The frozen head retains its learned parameters while other parts of the network are updated. But this means the frozen head better be pre-trained already in order to be useful during inference.\n",
        "\n",
        "**Advantages:** Can be great when we want to retain performance/prior leanred behavior  of frozen task head only and want the rest of the network to adapt to the new task. In terms of computational costs, it can be less costly relatively as well. This technique can als be use to prevent overfitting for the frozen task.\n",
        "\n",
        "**How it should be trained:**\n",
        "In setting the hyperparameters I'll consider the following:\n",
        "\n",
        "*  I'll set a low learning rate for the backbone and the trainable head e.g. 1e-5 to 4e-4\n",
        "*  Set the weight decay within 1e-4 to 1e-2 to ensure regularization of the backbone and trainable head.\n",
        "*  Set a linear decay scheduler with warmup for the backbone\n",
        "* use smaller batch sizes since we computing gradients for the backbone.\n",
        "\n",
        "During the trials, I'll gradually unfreeze the trainable head for a few epochs, increase the learning rate in the initial steps to avoid destabilizing the backbone and use optuna or hyperband for hyperparameter search since tuning both the backbone and a task head can get complex\n"
      ],
      "metadata": {
        "id": "MVRHhOCdKLPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning"
      ],
      "metadata": {
        "id": "GJWt-R8dXsGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choice of Pre-trained Model** I'll use either sentence bert or bert or roberta. My choice of which bert variant I\"ll use depends on the task at hand and demands of the task. If it's a case where I need just general text understanding, BERT would suffice, if it's a case where the texts involve specialized domains like science or sports or research or medicine, I'll use SciBERT or BioBERT or look for any of any BERT that has been optimized to encode sentences within that domain and finally if the task requieres high-qulity sentence embeddings, I'll use sentence-bert. In summary the pretrained model I'll consider using would be an encoder as the backbone, because this is what encoders are generally great at; creating great token embeddings that captures the context and semantics.\n",
        "\n",
        "**Which layers I'll freeze:** I'll start by freezing the entire transformer backbone. The rationale behind this is because the backbone already creates embeddings that generalizes well to tasks like sentence classification, etc. This would reduce overfitting on the dataset that would be used to implement the multi-task heads. After some epochs of training the other task heads on the dataset, I'll then gradually unfreeze the later layers of the transformer backbone and fine-tune them as these layers can be useful in capturing the task-specific features"
      ],
      "metadata": {
        "id": "hTIRkW9nXkE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task4: Layer-wise Learning Rate"
      ],
      "metadata": {
        "id": "F4JkmFMYYwLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 4 layers to implement layerwise learning rate are:\n",
        "* The early layers (closer to input) of the transformer backbone,\n",
        "* the later layers of the backbone,\n",
        "* the task head layers and\n",
        "* the embeddings layer/layer before our mean pooling of the backbone\n",
        "\n",
        "For the **early/deeper layers of the backbone**, *very low learning rates* are used to minimize the intensity of the gradient update and also to preserve the rigor of the backbone as those layers perform well in language understanding.\n",
        "\n",
        "For the **later layers of the backbone**, we can *increase the learning rate* used for the earlier layers a little more to encourage faster learning as these layers are closer to the task heads and can update their weights based on those tasks.\n",
        "\n",
        "For the **task head layers**, *high learning rates to encourage faster learning* because they need to adapt quickly to the tasks they are trained on\n",
        "\n",
        "A *low learning rate* is used in the **embedding layer** because its shared across both tasks and requires little to no changes during training"
      ],
      "metadata": {
        "id": "sjkbZHIpeaqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Potential Benefits**:\n",
        "\n",
        "Layer-wise learning rates can help with stability in training. This simply means we control the rate of learning across the network removing the need to freeze some layers. This can be crucial so as to preserve those already optimal weights at the early layers of the backbone that already carry good general language understanding and need not to be too finetuned to on any new tasks. We can basically control the rate of learning across several layers and increase learning rates for later layers as these layers need to adapt to the task.\n",
        "\n",
        "Layer-wise learning rates can also help with computational costs and efficiency. It can also introduce some regularization because, since we limit the rate of change of the weights in some of these layers, we can reduce the risk of overfitting.\n",
        "\n",
        "Layer-wise learning rates can also help preserve domain knowledge coming from the trasnformer backbone which can be super useful during finetuning. The model will balance preserving those weights and adapting to domain-specific features."
      ],
      "metadata": {
        "id": "_hkA0hkDiiTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define different learning rates for each layer\n",
        "def implement_layerwise_lr(model, base_lr=1e-5, layer_decay=0.9, head_lr=1e-4):\n",
        "    \"\"\"Applies layer-wise learning rates with an exponential decay.\"\"\"\n",
        "    optimizer_grouped_parameters = [] # initialize paramters list\n",
        "\n",
        "    # Get transformer backbone (BERT layers)\n",
        "    bert_layers = list(model.bert.encoder.layer)  # List of all BERT layers\n",
        "    for i, layer in enumerate(bert_layers):\n",
        "        lr = base_lr * (layer_decay ** (len(bert_layers) - i - 1))  # Later layers get higher lr\n",
        "        optimizer_grouped_parameters.append({\"params\": layer.parameters(), \"lr\": lr})\n",
        "\n",
        "    # Get embeddings layer and edit lr\n",
        "    optimizer_grouped_parameters.append({\"params\": model.bert.embeddings.parameters(), \"lr\": base_lr * (layer_decay ** len(bert_layers))})\n",
        "\n",
        "    # Task-specific heads\n",
        "    optimizer_grouped_parameters.append({\"params\": model.classification_head.parameters(), \"lr\": head_lr})\n",
        "    optimizer_grouped_parameters.append({\"params\": model.sentiment_head.parameters(), \"lr\": head_lr})\n",
        "\n",
        "    return optimizer_grouped_parameters\n",
        "\n",
        "# Init the optimizer with layer-wise learning rates and params\n",
        "model = MultiTaskModel(num_classes_task1=4, num_classes_task2=3)\n",
        "optimizer = torch.optim.AdamW(implement_layerwise_lr(model))"
      ],
      "metadata": {
        "id": "gqSeKSBvXhPV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "044939d2ee33471eb776370c0d867529",
            "07b9e1db6974478a82a38436ba249858",
            "c51ddfb73af840eb9d6af9f2dfdd8386",
            "f21bcdea8d8b4652af6b77f6938ce4a8",
            "b63cf1d036e445369f37b027a0a01048",
            "18510caaad6a472c927291d1fb573891",
            "42bf04d2caf244588c4ebec512ab6313",
            "16964ba827d84c32a19edcd15b6c17d8",
            "54f676f5708b4f618a285804a2e3f5b5",
            "802a27cbbdaf42fe8bab77014bbfa665",
            "f0d29b1b861e4ac99f58c846cb7649c7"
          ]
        },
        "outputId": "86329d96-70be-4120-ca24-a76b5275d4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "044939d2ee33471eb776370c0d867529"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_task1 = nn.CrossEntropyLoss()\n",
        "criterion_task2 = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "num_epochs = 6\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_total_loss = 0\n",
        "    val_total_loss = 0\n",
        "    correct_task1 = 0 # for accuracy calculation\n",
        "    correct_task2 = 0 # for accuracy calculation\n",
        "    total_samples = 0 # for accuracy calculation\n",
        "\n",
        "    # train\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels_task1 = batch[\"label_task1\"].to(device)\n",
        "        labels_task2 = batch[\"label_task2\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        classification_logits, sentiment_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute losses for each task\n",
        "        loss_task1 = criterion_task1(classification_logits, labels_task1)\n",
        "        loss_task2 = criterion_task2(sentiment_logits, labels_task2)\n",
        "\n",
        "        # Combine the losses\n",
        "        combined_loss = loss_task1 + loss_task2\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_total_loss += combined_loss.item()\n",
        "\n",
        "    train_avg_loss = train_total_loss / len(train_dataloader)\n",
        "\n",
        "    # eval\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      for batch in val_dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels_task1 = batch[\"label_task1\"].to(device)\n",
        "        labels_task2 = batch[\"label_task2\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        classification_logits, sentiment_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_task1 = criterion_task1(classification_logits, labels_task1)\n",
        "        loss_task2 = criterion_task2(sentiment_logits, labels_task2)\n",
        "        combined_loss = loss_task1 + loss_task2\n",
        "        val_total_loss += combined_loss.item()\n",
        "\n",
        "        # Calculate predictions\n",
        "        predictions_task1 = torch.argmax(classification_logits, dim=1)\n",
        "        predictions_task2 = torch.argmax(sentiment_logits, dim=1)\n",
        "\n",
        "        # Update correct counts for accuracy calculation\n",
        "        correct_task1 += (predictions_task1 == labels_task1).sum().item()\n",
        "        correct_task2 += (predictions_task2 == labels_task2).sum().item()\n",
        "        total_samples += labels_task1.size(0)\n",
        "\n",
        "      # Calculate average loss and accuracy\n",
        "      val_avg_loss = val_total_loss / len(val_dataloader)\n",
        "      accuracy_task1 = correct_task1 / total_samples\n",
        "      accuracy_task2 = correct_task2 / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_avg_loss:.4f}, Val Loss: {val_avg_loss:.4f}, Accuracy Task 1: {accuracy_task1:.4f}, Accuracy Task 2: {accuracy_task2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6nFxGfLhXN9",
        "outputId": "ad38a32f-d214-4495-b388-4df1e300118f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6, Train Loss: 2.5181, Val Loss: 2.5300, Accuracy Task 1: 0.3333, Accuracy Task 2: 0.3333\n",
            "Epoch 2/6, Train Loss: 2.3565, Val Loss: 2.5379, Accuracy Task 1: 0.0000, Accuracy Task 2: 0.3333\n",
            "Epoch 3/6, Train Loss: 2.2526, Val Loss: 2.5561, Accuracy Task 1: 0.0000, Accuracy Task 2: 0.3333\n",
            "Epoch 4/6, Train Loss: 2.1142, Val Loss: 2.5756, Accuracy Task 1: 0.0000, Accuracy Task 2: 0.3333\n",
            "Epoch 5/6, Train Loss: 2.0544, Val Loss: 2.5957, Accuracy Task 1: 0.0000, Accuracy Task 2: 0.3333\n",
            "Epoch 6/6, Train Loss: 1.9560, Val Loss: 2.6176, Accuracy Task 1: 0.0000, Accuracy Task 2: 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this solution, I purposely used a small sample dataset and a light weight model just to demo the implementation. I was more focused on building out the model and answering the questions than building something big."
      ],
      "metadata": {
        "id": "Wuqg4PilqS91"
      }
    }
  ]
}